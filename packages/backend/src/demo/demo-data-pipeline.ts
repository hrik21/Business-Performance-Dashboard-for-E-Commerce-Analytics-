/**
 * Demo Script for Data Pipeline Foundation
 * This script demonstrates the key features of the implemented data pipeline
 */

import { DataConnector, DataSourceConfig } from '../services/data-connector.service';
import { ETLPipelineService, ETLJobConfig } from '../services/etl-pipeline.service';
import { DataValidationService } from '../services/data-validation.service';
import { StreamProcessor, StreamProcessorConfig } from '../services/stream-processor.service';
import { DataQualityMonitorService } from '../services/data-quality-monitor.service';
import { ErrorHandlerService } from '../services/error-handler.service';
import { DatabaseHealthService } from '../services/database-health.service';
import { testDatabaseConnection, getDatabaseHealth } from '../config/database';
import { getKafkaHealth } from '../config/kafka';

export class DataPipelineDemo {
    private dataConnector: DataConnector;
    private etlService: ETLPipelineService;
    private validationService: DataValidationService;
    private qualityMonitor: DataQualityMonitorService;
    private errorHandler: ErrorHandlerService;
    private dbHealthService: DatabaseHealthService;

    constructor() {
        this.dataConnector = new DataConnector();
        this.etlService = new ETLPipelineService(this.dataConnector);
        this.validationService = new DataValidationService();
        this.qualityMonitor = new DataQualityMonitorService();
        this.errorHandler = new ErrorHandlerService();
        this.dbHealthService = DatabaseHealthService.getInstance();
    }

    /**
     * Run complete demo showcasing all features
     */
    async runCompleteDemo(): Promise<void> {
        console.log('üöÄ Starting E-commerce BI Data Pipeline Demo\n');

        try {
            // 1. Database Connection Demo
            await this.demonstrateDatabase();

            // 2. Data Connector Demo
            await this.demonstrateDataConnector();

            // 3. Data Validation Demo
            await this.demonstrateDataValidation();

            // 4. ETL Pipeline Demo
            await this.demonstrateETLPipeline();

            // 5. Error Handling Demo
            await this.demonstrateErrorHandling();

            // 6. Data Quality Monitoring Demo
            await this.demonstrateDataQuality();

            // 7. Stream Processing Demo
            await this.demonstrateStreamProcessing();

            // 8. System Health Demo
            await this.demonstrateSystemHealth();

            console.log('\n‚úÖ Demo completed successfully!');
            console.log('\nüìä Summary of Capabilities Demonstrated:');
            console.log('   ‚Ä¢ Database connectivity and health monitoring');
            console.log('   ‚Ä¢ Multi-source data ingestion (API, File, Database)');
            console.log('   ‚Ä¢ Data validation and cleansing');
            console.log('   ‚Ä¢ ETL pipeline with transformations');
            console.log('   ‚Ä¢ Error handling with retry mechanisms');
            console.log('   ‚Ä¢ Real-time data quality monitoring');
            console.log('   ‚Ä¢ Stream processing with Kafka');
            console.log('   ‚Ä¢ Comprehensive system health checks');

        } catch (error) {
            console.error('‚ùå Demo failed:', error);
        }
    }

    private async demonstrateDatabase(): Promise<void> {
        console.log('üìä 1. Database Connection & Health Monitoring');
        console.log('='.repeat(50));

        // Test database connection
        const isConnected = await testDatabaseConnection();
        console.log(`Database Connection: ${isConnected ? '‚úÖ Connected' : '‚ùå Failed'}`);

        // Get database health
        const dbHealth = await getDatabaseHealth();
        console.log(`Database Status: ${dbHealth.status}`);
        console.log(`Database Latency: ${dbHealth.latency}ms`);

        // Get detailed health metrics
        const healthStatus = await this.dbHealthService.checkHealth();
        console.log(`Connection Pool - Active: ${healthStatus.connections.active}, Idle: ${healthStatus.connections.idle}`);

        // Get database statistics
        const stats = await this.dbHealthService.getDatabaseStats();
        console.log(`Database Tables: ${stats.tableCount}`);
        console.log(`Database Size: ${stats.totalSize}`);
        console.log(`Active Connections: ${stats.connectionCount}`);

        console.log('');
    }

    private async demonstrateDataConnector(): Promise<void> {
        console.log('üîå 2. Multi-Source Data Connector');
        console.log('='.repeat(50));

        // Register different data sources
        const apiSource: DataSourceConfig = {
            id: 'demo-api',
            name: 'Demo API Source',
            type: 'api',
            config: {
                baseUrl: 'https://jsonplaceholder.typicode.com',
                endpoints: { users: '/users' },
                timeout: 5000,
            },
        };

        const fileSource: DataSourceConfig = {
            id: 'demo-file',
            name: 'Demo File Source',
            type: 'file',
            config: {
                path: '/tmp/demo-data.json',
                format: 'json',
            },
        };

        try {
            await this.dataConnector.registerDataSource(apiSource);
            console.log('‚úÖ API data source registered');

            await this.dataConnector.registerDataSource(fileSource);
            console.log('‚úÖ File data source registered');
        } catch (error) {
            console.log('‚ö†Ô∏è  Data source registration (expected in demo environment)');
        }

        // Show connection status
        const status = this.dataConnector.getConnectionStatus();
        console.log('üìã Registered Data Sources:');
        Object.entries(status).forEach(([id, info]) => {
            console.log(`   ‚Ä¢ ${info.name} (${info.type}): ${info.status}`);
        });

        console.log('');
    }

    private async demonstrateDataValidation(): Promise<void> {
        console.log('‚úÖ 3. Data Validation & Cleansing');
        console.log('='.repeat(50));

        // Sample data with quality issues
        const sampleData = [
            { name: '  John Doe  ', email: 'john@example.com', age: 30, revenue: 1500.50 },
            { name: 'Jane Smith', email: 'JANE@EXAMPLE.COM', age: 25, revenue: 2000 },
            { name: '', email: 'invalid-email', age: -5, revenue: null },
            { name: 'Bob Wilson', email: 'bob@test.com', age: 150, revenue: 'invalid' },
        ];

        // Define validation rules
        const validationRules = [
            { field: 'name', type: 'required' as const, config: {} },
            { field: 'email', type: 'pattern' as const, config: { pattern: '^[^@]+@[^@]+\\.[^@]+$' } },
            { field: 'age', type: 'range' as const, config: { min: 0, max: 120 } },
            { field: 'revenue', type: 'type' as const, config: { type: 'number' } },
        ];

        console.log('üìù Sample Data (with quality issues):');
        sampleData.forEach((record, index) => {
            console.log(`   ${index + 1}. ${JSON.stringify(record)}`);
        });

        // Validate dataset
        const { results, qualityReport } = await this.validationService.validateDataset(sampleData, validationRules);

        console.log('\nüìä Data Quality Report:');
        console.log(`   Total Records: ${qualityReport.totalRecords}`);
        console.log(`   Valid Records: ${qualityReport.validRecords}`);
        console.log(`   Invalid Records: ${qualityReport.invalidRecords}`);
        console.log(`   Validation Rate: ${qualityReport.validationRate.toFixed(2)}%`);

        // Show common errors
        if (qualityReport.commonErrors.length > 0) {
            console.log('\nüö® Common Validation Errors:');
            qualityReport.commonErrors.slice(0, 3).forEach(error => {
                console.log(`   ‚Ä¢ ${error.error} (${error.count} occurrences)`);
            });
        }

        // Demonstrate data cleansing
        const cleansedData = this.validationService.cleanseData(sampleData, ['email']);
        console.log('\nüßπ Data After Cleansing:');
        cleansedData.slice(0, 2).forEach((record, index) => {
            console.log(`   ${index + 1}. ${JSON.stringify(record)}`);
        });

        console.log('');
    }

    private async demonstrateETLPipeline(): Promise<void> {
        console.log('‚öôÔ∏è 4. ETL Pipeline Processing');
        console.log('='.repeat(50));

        // Define ETL job configuration
        const etlJob: ETLJobConfig = {
            id: 'demo-etl-job',
            name: 'Demo Sales Data ETL',
            description: 'Process and transform sales data',
            source: {
                dataSourceId: 'demo-api',
                query: 'SELECT * FROM sales_data',
                batchSize: 100,
            },
            transformations: [
                {
                    type: 'map',
                    config: {
                        mapping: {
                            customer_name: 'name',
                            customer_email: 'email',
                            order_total: 'revenue',
                        },
                    },
                    description: 'Map fields to standard schema',
                },
                {
                    type: 'filter',
                    config: {
                        conditions: [
                            { field: 'order_total', operator: 'greater_than', value: 0 },
                        ],
                    },
                    description: 'Filter out invalid orders',
                },
            ],
            destination: {
                table: 'processed_sales',
                mode: 'upsert',
                keyColumns: ['customer_email'],
            },
            validation: {
                enabled: true,
                rules: [
                    { field: 'customer_name', type: 'required', config: {} },
                    { field: 'customer_email', type: 'pattern', config: { pattern: '^[^@]+@[^@]+\\.[^@]+$' } },
                ],
                onFailure: 'skip',
            },
        };

        // Register ETL job
        this.etlService.registerJob(etlJob);
        console.log('‚úÖ ETL job registered successfully');

        // Show job configuration
        const jobs = this.etlService.getJobs();
        console.log(`üìã Registered ETL Jobs: ${jobs.length}`);
        jobs.forEach(job => {
            console.log(`   ‚Ä¢ ${job.name}: ${job.transformations.length} transformations`);
        });

        // Simulate job execution (would normally process real data)
        console.log('\nüîÑ ETL Job Execution Simulation:');
        console.log('   1. ‚úÖ Data extraction from source');
        console.log('   2. ‚úÖ Field mapping transformation applied');
        console.log('   3. ‚úÖ Data filtering completed');
        console.log('   4. ‚úÖ Validation rules applied');
        console.log('   5. ‚úÖ Data loaded to destination');

        console.log('');
    }

    private async demonstrateErrorHandling(): Promise<void> {
        console.log('üõ°Ô∏è 5. Error Handling & Resilience');
        console.log('='.repeat(50));

        // Demonstrate retry mechanism
        console.log('üîÑ Retry Mechanism Demo:');

        let attempts = 0;
        const flakyOperation = async () => {
            attempts++;
            if (attempts < 3) {
                throw new Error('Temporary connection failure');
            }
            return 'Operation successful';
        };

        try {
            const result = await this.errorHandler.executeWithRetry('demo-operation', flakyOperation, {
                maxRetries: 3,
                initialDelay: 100,
                maxDelay: 1000,
                backoffMultiplier: 2,
                retryableErrors: ['Temporary connection failure'],
            });
            console.log(`   ‚úÖ ${result} (after ${attempts} attempts)`);
        } catch (error) {
            console.log(`   ‚ùå Operation failed: ${error}`);
        }

        // Show error statistics
        const errorStats = this.errorHandler.getAllErrorStats();
        console.log('\nüìä Error Statistics:');
        Object.entries(errorStats).forEach(([operation, stats]) => {
            console.log(`   ‚Ä¢ ${operation}: ${stats.count} errors`);
        });

        // Demonstrate circuit breaker
        console.log('\n‚ö° Circuit Breaker Pattern:');
        console.log('   ‚Ä¢ Monitors failure rates');
        console.log('   ‚Ä¢ Opens circuit after threshold failures');
        console.log('   ‚Ä¢ Provides fast-fail responses');
        console.log('   ‚Ä¢ Automatically attempts recovery');

        console.log('');
    }

    private async demonstrateDataQuality(): Promise<void> {
        console.log('üìà 6. Real-time Data Quality Monitoring');
        console.log('='.repeat(50));

        // Process sample messages for quality monitoring
        const sampleMessages = [
            {
                key: 'order-1',
                value: { customerId: 'C001', amount: 150.00, email: 'customer1@example.com' },
                timestamp: new Date(),
            },
            {
                key: 'order-2',
                value: { customerId: 'C002', amount: 200.50, email: 'customer2@example.com' },
                timestamp: new Date(),
            },
            {
                key: 'order-3',
                value: { customerId: '', amount: -50, email: 'invalid-email' },
                timestamp: new Date(),
            },
        ];

        console.log('üìù Processing sample messages for quality analysis...');
        sampleMessages.forEach((message, index) => {
            this.qualityMonitor.processMessage(message, {
                success: index < 2, // First two succeed, third fails
                originalMessage: message,
            });
        });

        // Trigger metrics update
        (this.qualityMonitor as any).updateMetrics();

        // Generate quality report
        const qualityReport = this.qualityMonitor.generateReport();
        console.log('\nüìä Data Quality Metrics:');
        console.log(`   Overall Score: ${qualityReport.overallScore.toFixed(2)}%`);
        console.log(`   Completeness: ${qualityReport.trends.completeness.toFixed(2)}%`);
        console.log(`   Accuracy: ${qualityReport.trends.accuracy.toFixed(2)}%`);
        console.log(`   Validity: ${qualityReport.trends.validity.toFixed(2)}%`);

        // Show active alerts
        const alerts = this.qualityMonitor.getActiveAlerts();
        if (alerts.length > 0) {
            console.log('\nüö® Active Quality Alerts:');
            alerts.slice(0, 3).forEach(alert => {
                console.log(`   ‚Ä¢ ${alert.severity.toUpperCase()}: ${alert.message}`);
            });
        }

        // Show recommendations
        if (qualityReport.recommendations.length > 0) {
            console.log('\nüí° Quality Improvement Recommendations:');
            qualityReport.recommendations.slice(0, 3).forEach(rec => {
                console.log(`   ‚Ä¢ ${rec}`);
            });
        }

        console.log('');
    }

    private async demonstrateStreamProcessing(): Promise<void> {
        console.log('üåä 7. Real-time Stream Processing');
        console.log('='.repeat(50));

        // Stream processor configuration
        const streamConfig: StreamProcessorConfig = {
            id: 'demo-stream-processor',
            name: 'Demo Sales Stream Processor',
            inputTopic: 'sales-events',
            outputTopic: 'processed-sales',
            consumerGroupId: 'demo-group',
            processingRules: [
                {
                    type: 'transform',
                    config: {
                        fieldMappings: {
                            processedAmount: 'amount',
                            customerRef: 'customerId',
                        },
                        calculations: {
                            tax: { type: 'multiply', fields: ['amount'], multiplier: 0.1 },
                        },
                    },
                    description: 'Transform and calculate tax',
                },
                {
                    type: 'enrich',
                    config: {
                        addTimestamp: true,
                        addMetadata: true,
                    },
                    description: 'Enrich with metadata',
                },
            ],
            validation: {
                enabled: true,
                rules: [
                    { field: 'customerId', type: 'required', config: {} },
                    { field: 'amount', type: 'range', config: { min: 0, max: 10000 } },
                ],
                onFailure: 'skip',
            },
        };

        const streamProcessor = new StreamProcessor(streamConfig);

        console.log('‚öôÔ∏è Stream Processor Configuration:');
        console.log(`   ‚Ä¢ Input Topic: ${streamConfig.inputTopic}`);
        console.log(`   ‚Ä¢ Output Topic: ${streamConfig.outputTopic}`);
        console.log(`   ‚Ä¢ Processing Rules: ${streamConfig.processingRules.length}`);
        console.log(`   ‚Ä¢ Validation: ${streamConfig.validation?.enabled ? 'Enabled' : 'Disabled'}`);

        // Show processing capabilities
        console.log('\nüîÑ Stream Processing Capabilities:');
        console.log('   ‚Ä¢ Real-time message transformation');
        console.log('   ‚Ä¢ Data validation and filtering');
        console.log('   ‚Ä¢ Batch processing support');
        console.log('   ‚Ä¢ Error handling and dead letter queues');
        console.log('   ‚Ä¢ Performance monitoring and statistics');

        // Show statistics
        const stats = streamProcessor.getStats();
        console.log('\nüìä Processor Statistics:');
        console.log(`   ‚Ä¢ Messages Processed: ${stats.messagesProcessed}`);
        console.log(`   ‚Ä¢ Success Rate: ${stats.messagesSuccessful}/${stats.messagesProcessed}`);
        console.log(`   ‚Ä¢ Average Processing Time: ${stats.averageProcessingTime}ms`);
        console.log(`   ‚Ä¢ Throughput: ${stats.throughputPerSecond.toFixed(2)} msg/sec`);

        console.log('');
    }

    private async demonstrateSystemHealth(): Promise<void> {
        console.log('üè• 8. System Health & Monitoring');
        console.log('='.repeat(50));

        // Database health
        const dbHealth = await this.dbHealthService.checkHealth();
        console.log('üíæ Database Health:');
        console.log(`   Status: ${dbHealth.status}`);
        console.log(`   Latency: ${dbHealth.latency}ms`);
        console.log(`   Connections: ${dbHealth.connections.active} active, ${dbHealth.connections.idle} idle`);

        // Kafka health (will show error in demo environment)
        try {
            const kafkaHealth = await getKafkaHealth();
            console.log('\nüì° Kafka Health:');
            console.log(`   Status: ${kafkaHealth.status}`);
            console.log(`   Brokers: ${kafkaHealth.brokers.join(', ')}`);
            console.log(`   Topics: ${kafkaHealth.topics.length} available`);
        } catch (error) {
            console.log('\nüì° Kafka Health: ‚ö†Ô∏è  Not available in demo environment');
        }

        // System capabilities summary
        console.log('\nüéØ System Capabilities:');
        console.log('   ‚úÖ Multi-source data ingestion');
        console.log('   ‚úÖ Real-time stream processing');
        console.log('   ‚úÖ Data validation and quality monitoring');
        console.log('   ‚úÖ ETL pipeline with transformations');
        console.log('   ‚úÖ Error handling and resilience');
        console.log('   ‚úÖ Health monitoring and observability');
        console.log('   ‚úÖ Scalable and modular architecture');

        console.log('');
    }

    /**
     * Cleanup resources
     */
    async cleanup(): Promise<void> {
        await this.dataConnector.closeAllConnections();
        this.qualityMonitor.stopMonitoring();
    }
}

// Export for use in other modules
export default DataPipelineDemo;